# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #3 выполнил(а):
- Гусева Максима Дмитриевича
- РИ211121
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | # | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
### Реализовать систему машинного обучения в связке Python - Google-Sheets – Unity. При выполнении задания можно использовать видео-материалы и исходные данные, предоставленные преподавателями курса.
Ход работы: Работа над заданием началась с повторения настройки программ, с видео из методических указаний. работа с anaconda promt, и проведение работы с ии.
до запуска программы
![Image alt](https://github.com/GusevMaximDm/DA-in-GameDev-lab3/blob/main/200125677-c18b3b80-bd08-42fc-95c6-b41296a7e6ed.png)
после запуска
![Image alt](https://github.com/GusevMaximDm/DA-in-GameDev-lab3/blob/main/после%20работы.png)


- при большем количестве стендов с которыми можно работать также каждый раз уникальный результат


## Задание 2
### Подробно опишите каждую строку файла конфигурации нейронной сети, доступного в папке с файлами проекта по ссылке. Самостоятельно найдите информацию о компонентах Decision Requester, Behavior Parameters, добавленных на сфере.
trainer_type – задаёт тип используемого для обучения тренажёра (PPO или SAC)

batch_size – Количество опытов на каждой итерации градиентного спуска.

buffer_size – Количество опыта, которое нужно собрать для обновления итерации или её изучения.

learning_rate – изменение скорости обучения модели с течением времени.

beta – исследование случайных (более или менее) пространств действий.

epsilon – параметр влияющий на быстроту развития (ускорения работы) системы с каждой итерацией.

lambd – параметр оценивающий совпадение стоимости вознаграждения обучений между собой (что то вроде погрешности), чем стабильнее значения. тем быстрее идёт процесс.

num_epoch – Количество проходов через буфер опыта при выполнении оптимизации градиентного спуска. (чем больше, тем быстрее итерация и наоборот)

learning_rate_schedule – скорость обучения, которая в нашем случае уменьшается линейно до нуля.

normalize – отвечает за то будут ли нормироваться (усредняться) входные данные (в нашем случае -нет)

hidden_units – количество значений в подключенном слое нейронной сети (то есть чем больше будет поток входных данных, тем больше нужно ввести значения параметра и наоборот).

num_layers – Количество скрытых слоев в нейронной сети. Число слоёв, принимающих на себя работу, чем сложнее задача, тем больше нужно слоёв.

gamma – Параметр, отвечающий за то, насколько далеко вперёд должен думать о вознаграждении агент. Должен быть в состоянии подготовиться – выделить объём места и т.д. 

strength – Коэффициент, на который умножается вознаграждение, предоставляемое средой. Благодаря этому параметру можно увеличивать или уменьшать количество поступаемой валюты.

max_steps – Общее количество шагов-действий, которые должны быть выполнены в среде до завершения процесса обучения.

time_horizon – Сколько опыта нужно собрать перед тем, как добавить в буфер. Также используется, как среднее значение для общего ожидаемого вознаграждения.

summary_freq – Количество опыта, которое необходимо собрать перед созданием и отображением статистики обучения.

hyperparameters – Группировка параметров, отвечающих за управления процессом обучения

network_settings – Группировка параметров, отвечающих за обучение сети.

reward_signals – Раздел позволяющий задавать настройки как для внешних, так и для внутренних сигналов вознаграждения.

extrinsic – внешний сигнал из reward_signals

Decision Requester это компонент автоматически запрашивающий решение с постоянным интервалом времени. Тоесть он отвечает за принятие решения в цикле: наблюдение-принятия решения-действие-вознаграждение.

Behavior Parameters -это компонент выполняющий функции настройки поведения агента (генерирует объекты и их свойства согласно заданным параметрам).

## Выводы

в первом задании пронаблюдали взаимодействие двух объектов и пришли к выводу, что кол-во результатов безгранично. Во втором зднии познакомилсь с внушительным кол-вом терминов

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
